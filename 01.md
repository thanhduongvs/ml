# Linear regression with one variable

## Mô hình (hypothesis)
$\hat y = h_\theta(x) = \theta_0 + \theta_1 x$

## Hàm chi phí (cost function)
$J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^{m}\left(h_\theta(x^{(i)})-y^{(i)}\right)^2 = \frac{1}{2m}\sum_{i=1}^{m}\left(\theta_0+\theta_1 x^{(i)}-y^{(i)}\right)^2$

## Gradient Descent
- gradient descent algorithm  
$
\text{repeat until convergence}\;\Big\{
\;\theta_j := \theta_j - \alpha\,\frac{\partial}{\partial \theta_j}\,J(\theta_0,\theta_1)
\;\Big\},\quad (\text{for } j=0,1)
$

$\theta_0 \leftarrow \theta_0 - \alpha \cdot \frac{1}{m}\sum_{i=1}^{m}\left(h_\theta\!\left(x^{(i)}\right)-y^{(i)}\right)$

$\theta_1 \leftarrow \theta_1 - \alpha \cdot \frac{1}{m}\sum_{i=1}^{m}\left(h_\theta\!\left(x^{(i)}\right)-y^{(i)}\right)\,x^{(i)}$

## zz
$
x=
\begin{bmatrix}
x^{(0)}\\
x^{(1)}\\
x^{(2)}\\
\vdots\\
x^{(m-1)}
\end{bmatrix}\!\in\mathbb{R}^{m\times 1},
\qquad
y=
\begin{bmatrix}
y^{(0)}\\
y^{(1)}\\
y^{(2)}\\
\vdots\\
y^{(m-1)}
\end{bmatrix}\!\in\mathbb{R}^{m\times 1},
\qquad
\theta=
\begin{bmatrix}
\theta_0\\
\theta_1
\end{bmatrix}\!\in\mathbb{R}^{2\times 1}
$

$
\hat y = h_\theta(x) = \theta_0 + \theta_1 x = \begin{bmatrix} 1 & x \end{bmatrix}
\begin{bmatrix} \theta_0 \\ \theta_1 \end{bmatrix} =
\underbrace{\begin{bmatrix}
1&x^{(0)}\\
1&x^{(1)}\\
\vdots&\vdots\\
1&x^{(m-1)}
\end{bmatrix}}_{\tilde X\in\mathbb{R}^{m\times 2}}
\underbrace{\begin{bmatrix}\theta_0\\ \theta_1\end{bmatrix}}_{\theta}
$

$\underbrace{H}_{m\times 1} = \underbrace{X}_{m\times 2}\times \underbrace{\theta}_{2\times 1}
$

% (tùy chọn) dạng vector hoá cho toàn bộ m mẫu
$$
\hat{\mathbf y}=\theta_0\,\mathbf 1_m+\theta_1\,X \in \mathbb{R}^{m\times 1},
\quad\text{hoặc}\quad
\hat{\mathbf y}=
\underbrace{\begin{bmatrix}
1&x^{(0)}\\
1&x^{(1)}\\
\vdots&\vdots\\
1&x^{(m-1)}
\end{bmatrix}}_{\tilde X\in\mathbb{R}^{m\times 2}}
\underbrace{\begin{bmatrix}\theta_0\\ \theta_1\end{bmatrix}}_{\theta}.
$$


$$
X=
\begin{bmatrix}
1 & x^{(0)}\\
1 & x^{(1)}\\
\vdots & \vdots\\
1 & x^{(m-1)}
\end{bmatrix}\in\mathbb{R}^{m\times 2},
\qquad
\theta=
\begin{bmatrix}\theta_0\\ \theta_1\end{bmatrix}\in\mathbb{R}^{2\times 1},
\qquad
Y=
\begin{bmatrix}
y^{(0)}\\
y^{(1)}\\
\vdots\\
y^{(m-1)}
\end{bmatrix}\in\mathbb{R}^{m\times 1}.
$$

$$
H = X\,\theta \in \mathbb{R}^{m\times 1},
\qquad
\hat y^{(i)}=\theta_0+\theta_1\,x^{(i)}.
$$

$
\theta = \theta-\frac{\alpha}{m}\,X^{\top}(H-Y)
\quad\text{(batch gradient descent)}
$

$
\theta=(X^{\top}X)^{-1}X^{\top}Y
\quad\text{(normal equation)}
$

$
J(\theta)=\frac{1}{2m}\,(H-Y)^{\top}(H-Y)
$